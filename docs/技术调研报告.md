# 技术调研报告

## 1. 技术方案

### 1.1 核心技术栈

-   **编程语言**: Python 3
-   **数据处理与分析**: Pandas
-   **数据获取**: Akshare
-   **数据库**: SQLite
-   **数据库交互**: SQLAlchemy
-   **数据可视化**: Matplotlib
-   **依赖管理**: `requirements.txt`

### 1.2 技术选型理由

-   **Python 3**: 作为数据科学领域的首选语言，其丰富的生态（如 `pandas`）能极大提升开发效率。
-   **Pandas**: 处理表格数据的标准库，`DataFrame` 对象非常适合财务数据的存储、清洗和计算。
-   **Akshare**: 已在需求澄清阶段确定，提供便捷的东方财富数据源接口。
-   **SQLite**: 轻量级、基于文件的数据库，无需配置，非常适合本地单用户应用，且性能足以应对预估数据量。
-   **SQLAlchemy**: 强大的ORM工具，能用Python对象操作数据库，简化代码，并使未来更换数据库变得容易，满足可扩展性需求。
-   **Matplotlib**: 功能强大的基础绘图库，能精确控制图表细节，满足静态图表生成的需求。配合 `mpld3` 或直接转换为HTML嵌入，实现HTML报告输出。
-   **Tqdm**: 进度条显示库，用于数据更新和计算过程的进度反馈。
-   **requirements.txt**: Python项目依赖管理标准，确保环境一致性。

### 1.3 替代方案

-   **数据库**:
    -   **DuckDB**: 一个专为分析查询设计的列式存储数据库。查询速度通常快于 SQLite，但对于本项目这种写入一次、多次读取的场景，SQLite 的成熟性和广泛性更具优势。
    -   **直接使用文件存储 (CSV/Parquet)**: 简单直接，但难以进行高效的索引和增量更新，不便于管理。
-   **数据库交互**:
    -   **直接编写 SQL**: 更灵活，但代码可读性和可维护性较差，且与特定数据库绑定，不利于扩展。

## 2. 数据规模估算

### 2.1 数据量估算

-   **公司数量**: ~5,500 家 (A股)
-   **报告频率**: 季度
-   **时间跨度**: 平均 **15 年** (约 60 个报告期)，考虑到所选接口将获取公司上市以来的全部历史数据，此为保守估计。
-   **财务报表**: 3 张 (资产负债表, 利润表, 现金流量表)
-   **每张报表字段数 (估算)**: ~250 个
-   **总数据行数 (每张表)**: 5,500 公司 × 60 报告期 = 330,000 行
-   **总数据点数 (估算)**: 330,000 行 × 250 字段 × 3 表 ≈ **2.48 亿**

### 2.2 存储空间估算

-   **数据库 (SQLite)**: 预计占用 **3-5 GB**。
-   **项目总空间**: 整个项目（代码、依赖、数据库）预计**不超过 7 GB**。

### 2.3 性能瓶颈预估

-   **数据获取**: API 接口可能存在调用频率限制，一次性获取全量数据耗时过长或失败。
-   **数据计算**: 对全市场数据进行聚合计算时，一次性将数 GB 数据加载到内存可能导致内存溢出。
-   **数据库交互**: 对大表进行频繁的全表扫描或复杂的连接查询会降低响应速度。

## 3. 可行性分析

### 3.1 技术可行性

-   所选技术栈均为成熟、主流的开源方案，社区活跃，文档齐全。
-   预估的数据规模在所选技术栈（Pandas, SQLite）的处理能力范围之内。但修正后的数据显示数据量较大，因此**必须**在开发中严格执行风险应对策略（如分块计算、索引优化），以保证程序性能和稳定性。
-   项目作为本地应用程序运行，不涉及复杂的部署和运维，技术实现难度可控。

### 3.2 性能优化策略

#### 3.2.1 内存管理

**目标**: 确保程序能在8GB内存的机器上稳定运行。

**具体措施**:
-   **内存上限**: 单次加载数据不超过2GB（为系统和计算预留空间）
-   **分块读取**: 使用 `pandas.read_sql` 的 `chunksize` 参数，每次处理500-1000只股票的数据
-   **分组计算**: 计算全市场中位数时，按报告期分组处理，而非一次性加载所有数据
-   **及时释放**: 使用完大型DataFrame后及时删除并调用 `gc.collect()`

#### 3.2.2 进度反馈

**实施要求**: 所有需要用户等待的场景都需要显示进度条。

**具体实现**:
-   使用 `tqdm` 库实现进度条
-   `data_updater.py`: 显示 "已完成 X/5500 只股票"
-   `main.py` 计算中位数: 显示 "计算中位数 X/40 个报告期"
-   API调用失败时显示重试进度

#### 3.2.3 数据质量报告

**输出方式**: 独立的文本文件（如 `data_quality_report_YYYYMMDD_HHMMSS.txt`）

**报告内容**:
1.  更新概况（开始/结束时间、总耗时、尝试更新的股票总数）
2.  成功/失败统计（成功和失败的股票数量及列表）
3.  数据质量统计（各报表类型的数据完整性、平均字段缺失率）
4.  未映射列名列表
5.  缓存状态（缓存版本号、缓存失效情况）

### 3.3 潜在风险与应对措施

-   **风险1：API 接口变更**
    -   **影响**: 可能导致数据获取模块完全失效。
    -   **应对措施**: 将 API 的 URL、请求参数等信息存储在独立的配置文件中，而非硬编码。当接口变更时，只需修改配置，降低维护成本。

-   **风险2：API 频率限制**
    -   **影响**: 批量获取数据时可能因请求过于频繁而被服务器拒绝，导致任务中断。
    -   **应对措施**: 设计数据拉取模块时，采用分批获取（如按年份或按批次）并加入延时。同时实现断点续传机制，确保任务中断后能从断点继续。

-   **风险3：计算时内存溢出**
    -   **影响**: 在内存较小的机器上进行全市场分析时，程序可能因耗尽内存而崩溃。
    -   **应对措施**: 进行全市场数据分析时，使用 `pandas` 的 `chunksize` 功能分块读取和处理数据，有效控制内存峰值。

-   **风险4：数据质量问题**
    -   **影响**: 财务数据中可能存在的 `空值(NaN)` 或异常值会导致计算错误。
    -   **应对措施**: 建立标准化的数据清洗流程，在数据存入数据库前进行预处理。

## 4. 推荐方案

### 4.1 推荐技术栈

-   **语言**: Python 3
-   **核心库**: Pandas, SQLAlchemy, Akshare, Matplotlib
-   **数据库**: SQLite

### 4.2 推荐实施策略

1.  **分批获取与断点续传**: 实现健壮的数据获取模块，规避 API 限制风险。
2.  **索引优化**: 在数据库核心查询字段（`股票代码`, `报告日期`）上建立索引，提升查询性能。
3.  **分块计算与内存管理**: 采用分块处理策略（每次500-1000只股票），单次加载不超过2GB，保证程序在8GB内存机器上稳定运行。
4.  **配置化管理**: 使用 `config.yaml` 统一管理所有配置参数（API、数据库、性能、输出、日志等）。
5.  **进度反馈**: 使用 `tqdm` 库为所有长时间操作提供进度条显示。
6.  **数据质量监控**: 每次数据更新后自动生成数据质量报告文件。

## 5. 待确认事项

-   无
